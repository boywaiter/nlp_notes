{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_notes02.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/boywaiter/nlp_notes/blob/master/nlp_notes02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n8PJvbajLHm",
        "colab_type": "text"
      },
      "source": [
        "# Chapter 2 Linear Text Claasification\n",
        "\n",
        "**text classification**: given a text document, assign it a discrete label $y\\in\\mathcal{Y}$, where $\\mathcal Y$ is the set of possible labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_S-pkPKHVkP6",
        "colab_type": "text"
      },
      "source": [
        "## 2.1 The bag of words\n",
        "A document or an instance is represented as a column vector, $\\boldsymbol x=[0, 1, 1, 0, 0, 2, 0, 1, 13, 0 ,\\ldots]^\\textrm{T}$, where $x_j$ is the count of the $j$-th word. The length of $\\boldsymbol x$ is $V=|\\mathcal V|$, the size of vocabulary.\n",
        "\n",
        "**Bag of words**: contains only word count information, no order information of words.\n",
        "\n",
        "**Weights** $\\boldsymbol \\theta$ assigns for each word a score, measuring the compatability with the label.\n",
        "\n",
        "To predict the label $\\hat y$ for a given $\\boldsymbol x$, we compute a score $\\Psi(\\boldsymbol x, y)$ for each $y\\in \\mathcal Y$,\n",
        "$$\n",
        "\\Psi (\\boldsymbol x, y)= \\mathbf \\theta \\cdot \\boldsymbol f(\\boldsymbol x, y)=\\sum_{j=1}^V \\theta_j\\cdot f_j(\\boldsymbol x,y) \\tag{2.1}\n",
        "$$\n",
        "where, it may seem awkward, the $\\boldsymbol f(\\boldsymbol x, y)$ returns a column vector of length $K\\times V$ and with $(K-1)\\times V$ zeros,  \n",
        "$$\n",
        "{\\boldsymbol{f}(\\boldsymbol{x}, y=1)=[\\boldsymbol{x} ; \\underbrace{0 ; 0 ; \\ldots ; 0}_{(K-1)\\times V}] }\\\\ \\tag{2.3}\n",
        "$$\n",
        "$$\n",
        "{\\boldsymbol{f}(\\boldsymbol{x}, y=2)=[\\underbrace{0 ; 0 ; \\ldots ; 0 }_{V};\\boldsymbol{x} ; \\underbrace{0 ; 0 ; \\ldots ; 0}_{(K-2)\\times V}]}\\tag{2.4}\n",
        "$$\n",
        "$$\n",
        "{\\boldsymbol{f}(\\boldsymbol{x}, y=K)=[\\underbrace{0 ; 0 ; \\ldots ; 0}_{(K-1) \\times V} ; \\boldsymbol{x}]}\\tag{2.5}\n",
        "$$\n",
        "The weights $\\boldsymbol \\theta$ is thus a column vector of the length $K\\times V$, i.e., $\\boldsymbol \\theta\\in \\mathbb R^{KV}$, where $\\theta_{(k-1)*V+1:kV}$ are for label $y=k$. \n",
        "\n",
        "We predict the label $\\hat y$ with\n",
        "$$\n",
        "\\hat y =\\mathop{\\text{argmax}}\\limits_{y\\in \\mathcal Y} \\Psi(\\boldsymbol x,y)\\tag{2.6}\n",
        "$$\n",
        "$$\n",
        "\\Psi(\\boldsymbol x,y )=\\boldsymbol \\theta\\cdot\\boldsymbol f(\\boldsymbol x,y)\\tag{2.7}\n",
        "$$\n",
        "It is common to add an **offset feature** at the end of $\\boldsymbol x$, which is always 1. The weight for it can be thought of as a bias.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybolQlb2nrFE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMMhPHcBr6Ls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "K=3\n",
        "V=5\n",
        "x=torch.randint(10,(V,1))\n",
        "x=torch.cat((x,torch.ones(1,1,dtype=torch.long)),0)#offset features\n",
        "weights=torch.ones(K*(V+1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xzuf5jq7salE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def feature_function(x,y):\n",
        "  return {(y-1)*(V+1)+i:x[i] for i in range(0,V+1)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDsL37OipVbM",
        "colab_type": "code",
        "outputId": "fd0c8934-35f2-43a2-e3b2-48e7a02cbc02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(feature_function(x,2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{6: tensor([7]), 7: tensor([5]), 8: tensor([8]), 9: tensor([7]), 10: tensor([1]), 11: tensor([1])}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dg8wcWx6jAn7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_score(x,y,weights):\n",
        "  total=0\n",
        "  for feature, count in feature_function(x,y).items():\n",
        "    total+=weights[feature] * count\n",
        "  return total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tr1W6rTjpRaT",
        "colab_type": "code",
        "outputId": "eabd96f8-2063-4a85-a787-1184a5596060",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "print(x)\n",
        "print(weights)\n",
        "total=compute_score(x,2,weights)\n",
        "print(total)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[7],\n",
            "        [5],\n",
            "        [8],\n",
            "        [7],\n",
            "        [1],\n",
            "        [1]])\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
            "tensor([29.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0v-f5cKV0EO",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 Naive Bayes\n",
        "$\\textrm{p}(\\boldsymbol x, y)$ is the **joint probability** of a text instance. A dataset of $N$ labeled instances is $\\{(\\boldsymbol x^{(i)}, y^{(i)})\\}_{i=1}^N$. We assume these instances are **independent and identically distributed (IID)**, i.e., $\\textrm{p}(\\boldsymbol x, y)=\\prod_{i=1}^N \\textrm{p}(\\boldsymbol x^{(i)}, y^{(i)})$.\n",
        "\n",
        "**Maximum Likelihood Estimation**:\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "\\hat y &=& \\mathop{\\text{argmax}}\\limits_\\boldsymbol \\theta \\textrm{p}(\\boldsymbol x, y;\\boldsymbol \\theta)\\tag{2.8}\\\\\n",
        "&=&\\mathop{\\text{argmax}}\\limits_\\boldsymbol \\theta\\prod_{i=1}^N \\textrm{p}(\\boldsymbol x^{(i)}, y^{(i)};\\boldsymbol \\theta) \\tag{2.9}\\\\\n",
        "&=&\\mathop{\\text{argmax}}\\limits_\\boldsymbol \\theta\\sum_{i=1}^N \\log\\textrm{p}(\\boldsymbol x^{(i)}, y^{(i)};\\boldsymbol \\theta)\\tag{2.10}\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "The probability $\\textrm{p}(\\boldsymbol x^{(i)}, y^{(i)};\\boldsymbol \\theta)$ is defined through a generative model â€” an idealized\n",
        "random process that has generated the observed data. The joint probability is factored using the chain rule,\n",
        "$$\n",
        "\\textrm{p}_{X,Y}(\\boldsymbol x^{(i)}, y^{(i)};\\boldsymbol \\theta)=\\textrm{p}_{X|Y}(\\boldsymbol x^{(i)}| y^{(i)};\\boldsymbol \\phi)\\times \\textrm{p}_{Y}(y^{(i)};\\boldsymbol \\mu)\\tag{2.11}\n",
        "$$\n",
        "where $\\boldsymbol \\theta=\\{\\boldsymbol \\mu, \\boldsymbol \\phi\\}$.\n",
        "\n",
        "- The distribution for $\\text p_{X|Y}$ is the **multinomial**,\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "\\text p_\\text{mult}(\\boldsymbol x; \\boldsymbol \\phi)&=&B(\\boldsymbol x)\\prod_{j=1}^V \\phi_j^{x_j}\\tag{2.12}\\\\\n",
        "B(\\boldsymbol x)&=&\\frac{(\\sum_{j=1}^V x_j)!}{\\prod_{j=1}^V(x_j)!}\\tag{2.13}\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "where $\\phi_j$ is the probability $x_j$ occurs in $\\boldsymbol x$.\n",
        "- The distribution for $\\text p_{Y}$ is the **categorical**, $\\text p(y;\\boldsymbol \\mu)=\\mu_y$ with $\\boldsymbol \\mu=[\\mu_1; \\mu_2; \\ldots, \\mu_K]$ and $\\sum_{j=1}^K \\mu_j=1$.\n",
        "\n",
        "### 2.2.1 Types and tokens\n",
        "\n",
        "If we use a sequence of tokens $\\boldsymbol w=(w_1, w_2, \\ldots, w_M)$ instead of types $\\boldsymbol x=\\{x_j|x_j\\in\\{1, 2, \\ldots, M\\}\\}$ to represent a text instance, then the probability for it is a product of categorical probabilities, e.g., $\\text p(w_m)=\\phi_m$. \n",
        "$$\n",
        "\\text p(\\boldsymbol w;\\boldsymbol \\phi)=\\prod_{m=1}^M \\text p(w_m;\\phi_m)=\\prod_{m=1}^M \\phi_m=\\prod_{j=1}^V \\phi_j^{x_j}\n",
        "$$\n",
        "### 2.2.2 Prediction\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "\\hat y &=& \\mathop{\\text{argmax}}\\limits_y \\log \\textrm{p}(\\boldsymbol x, y;\\boldsymbol \\mu, \\boldsymbol \\phi)\\tag{2.14}\\\\\n",
        "&=& \\mathop{\\text{argmax}}\\limits_y \\log \\textrm{p}(\\boldsymbol x| y;\\boldsymbol \\phi)+\\log\\textrm{p}(y;\\boldsymbol \\mu)\\tag{2.15}\\\\\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "\\log \\textrm{p}(\\boldsymbol x| y;\\boldsymbol \\phi)+\\log\\textrm{p}(y;\\boldsymbol \\mu)&=&\\log \\left[B(\\boldsymbol x)\\prod_{j=1}^V \\phi_j^{x_j}\\right]+\\log \\mu_y\\tag{2.16}\\\\\n",
        "&=&\\log B(\\boldsymbol x)+\\sum_{j=1}^V x_j\\log \\phi_j+\\log \\mu_y\\tag{2.17}\\\\\n",
        "&=&\\log B(\\boldsymbol x)+\\boldsymbol \\theta \\cdot \\boldsymbol f(\\boldsymbol  x, y)\\tag{2.18}\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "where \n",
        "$$\\boldsymbol \\theta=[\\boldsymbol \\theta^{(1)}; \\boldsymbol \\theta^{(2)}; \\ldots; \\boldsymbol \\theta^{(K)}]\\tag{2.19}$$\n",
        "$$\\boldsymbol \\theta^{(y)}=[\\log \\phi_{y,1}; \\log \\phi_{y,2}; \\ldots ; \\log \\phi_{y,V};\\log \\mu_y]\\tag{2.20}$$\n",
        "and $\\boldsymbol f(\\boldsymbol  x, y)$ is shown in Equations 2.3-2.5. This is a key point: through this notation, we have converted the problem of computing the log-likelihood for a document-label pair (x; y) into the computation of a vector inner product.\n",
        "### 2.2.3 Estimation\n",
        "The parameters of the categorical and multinomial distributions have a simple interpretation: they are vectors of expected frequencies for each possible event.\n",
        "$$\n",
        "\\phi_y,j=\\frac{count(y,j)}{\\sum_{j'=1}^V count(y,j')}=\\frac{\\sum_{i:y^{(i)}=y}x^{(i)}_j}{\\sum_{j'=1}^V \\sum_{i:y^{(i)}=y}x^{(i)}_{j'}}\n",
        "$$\n",
        "where $count(y,j)$ refers to the count of word $j$ in documents labeled as $y$.\n",
        "\n",
        "Equation 2.21 defines the relative frequency estimate for $\\phi$. It can be justified as a **maximum likelihood estimate**.\n",
        "$$\n",
        "\\mathcal{L}(\\boldsymbol \\phi, \\boldsymbol \\mu)=\\sum_{i=1}^N \\log \\text p_{\\text{mult}}(\\boldsymbol x^{(i)}|y^{(i)};\\phi_{y^{(i)}})+\\log \\text p_{\\text{cat}}(y^{(i)};\\mu_{y^{(i)}})\\tag{2.22}\n",
        "$$\n",
        "$$\n",
        "\\mathcal{L}(\\boldsymbol \\phi)=\\sum_{i=1}^N \\log \\text p_{\\text{mult}}(\\boldsymbol x^{(i)}|y^{(i)};\\phi_{y^{(i)}})=\\sum_{i=1}^N \\left(\\log B(\\boldsymbol x^{(i)})+\\sum_{j=1}^Vx^{(i)}_j\\log\\phi_{y^{(i)}, j}\\right)\\tag{2.23}\n",
        "$$\n",
        "$$\n",
        "\\sum_{j=1}^V \\phi_{y,j}=1\\text{ } \\forall y \\tag{2.24}\n",
        "$$\n",
        "$$\n",
        "\\ell(\\phi_y)=\\sum_{i:y^{(i)}=y}^N\\sum_{j=1}^V x^{(i)}_j\\log\\phi_{y, j}-\\lambda(\\sum_{j=1}^V \\phi_{y,j}-1).\\tag{2.25}\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial \\ell(\\phi_y)}{\\partial \\phi_{y,j}}=\\sum_{i:y^{(i)}=y}^N x^{(i)}_j/\\phi_{y, j}-\\lambda\\tag{2.26}\n",
        "$$\n",
        "$$\n",
        "\\lambda\\phi_{y, j}=\\sum_{i:y^{(i)}=y}^N x^{(i)}_j\\tag{2.27}\n",
        "$$\n",
        "$$\n",
        "\\phi_{y, j}\\propto\\sum_{i:y^{(i)}=y}^N x^{(i)}_j=\\sum_{i=1}^N \\delta (y^{(i)}=y)x^{(i)}_j=count(y, j)\\tag{2.28}\n",
        "$$\n",
        "$$\n",
        "\\mathcal{L}(\\boldsymbol \\mu)=\\sum_{i=1}^N \\log \\text p_{\\text{cat}}(y^{(i)};\\mu_{y^{(i)}})=\\sum_{i=1}^N \\log \\mu_{y^{(i)}}=\\sum_{y\\in\\mathcal{Y}}\\sum_{i=1}^N \\delta (y^{(i)}=y)\\log \\mu_y $$\n",
        "$$\n",
        "\\sum_{y\\in\\mathcal{Y}}\\mu_y=1\n",
        "$$\n",
        "$$\n",
        "\\ell(\\boldsymbol\\mu)=\\sum_{y\\in\\mathcal{Y}}\\sum_{i=1}^N \\delta (y^{(i)}=y)\\log \\mu_y -\\lambda(\\sum_{y\\in\\mathcal{Y}}\\mu_y-1)\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial \\ell(\\boldsymbol\\mu)}{\\partial \\mu_y}=\\sum_{i=1}^N \\delta (y^{(i)}=y)/\\mu_y -\\lambda\n",
        "$$\n",
        "$$\n",
        "\\mu_y\\propto \\sum_{i=1}^N \\delta (y^{(i)}=y)\n",
        "$$\n",
        "### 2.2.4 Smoothing\n",
        "Given $\\phi_{FICTION, molybdenum}=0$, $\\log \\phi_{FICTION, molybdenum}=-\\infty$, which in turn makes $\\text p(FICTION|\\boldsymbol x)=0$.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9etGacWYI2oR",
        "colab_type": "text"
      },
      "source": [
        "## 2.3 Discriminative learning\n",
        "**Descriminative learning**, comparing to generative learning, avoid learning the joint probability of the training instance and its label, focuses directly on predicting the label.\n",
        "\n",
        "### 2.3.1 Perceptron\n",
        "**Online learning**: the classifier weights update after every example.\n",
        "\n",
        "**Batch learning**: computes statistics over the entire dataset and then sets the weights in a single operation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocxpovpcuJuL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Word_count_max=10\n",
        "N=10\n",
        "V=5\n",
        "K=2\n",
        "X=torch.randint(low=1,high=Word_count_max,size=(N,V))\n",
        "X=torch.cat((X,torch.ones(N,1,dtype=torch.long)),1)\n",
        "y=torch.randint(0,K,size=(N,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3ozlmioGkzx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "e2f32e4f-68f6-4147-8f79-1a232d321e1f"
      },
      "source": [
        "print(X)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[2, 1, 3, 2, 9, 1],\n",
            "        [5, 7, 3, 9, 7, 1],\n",
            "        [2, 9, 3, 7, 4, 1],\n",
            "        [6, 5, 3, 8, 1, 1],\n",
            "        [2, 1, 9, 9, 3, 1],\n",
            "        [8, 1, 5, 3, 1, 1],\n",
            "        [5, 9, 2, 3, 5, 1],\n",
            "        [3, 8, 8, 2, 2, 1],\n",
            "        [1, 8, 3, 1, 7, 1],\n",
            "        [7, 7, 8, 5, 8, 1]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQMtc5pZu9O4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "def feature_function_binary(x,y):\n",
        "  dict1={y*(V+1)+i:x[i].item() for i in range(0,V+1)}  \n",
        "  dict2={(1-y)*(V+1)+i:0 for i in range(0,V+1)}\n",
        "  dict3={}\n",
        "  dict3.update(dict1)\n",
        "  dict3.update(dict2)\n",
        "  return dict3\n",
        "def update_theta(theta,D1,D2):\n",
        "  for feature, count in D1.items():\n",
        "    #print(\"feature=\",feature,\" count=\",count)\n",
        "    theta[feature]+=count-D2[feature]\n",
        "  return theta\n",
        "def compute_score_binary(x,y,weights):\n",
        "  total=0\n",
        "  for feature, count in feature_function_binary(x,y).items():\n",
        "    #print(\"feature=\",feature)\n",
        "    #print(\"count=\",count)\n",
        "    total+=weights[feature] * count\n",
        "  return total\n",
        "def perceptron(X,y):\n",
        "  t=0\n",
        "  theta=torch.zeros(K*(V+1),dtype=torch.float)\n",
        "  #print(\"theta=\",theta)\n",
        "  times=100\n",
        "  while(t<times):\n",
        "    i= np.random.randint(0,N,1).item()\n",
        "    x=X[i]    \n",
        "    #print(\"x=\",x)\n",
        "    hat_y=0 if compute_score_binary(x,0,theta).item()>=compute_score_binary(x,1,theta).item() else 1\n",
        "    #print(\"hat_y=\",hat_y,\"y[i]=\",y[i].item())\n",
        "    if(hat_y!=y[i].item()):\n",
        "      theta=update_theta(theta,feature_function_binary(x,y[i].item()),\n",
        "                         feature_function_binary(x,hat_y))\n",
        "    t+=1\n",
        "  return theta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7qAZTH-Gji6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBDlffpuMxes",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "58494928-ca3b-47e3-da30-8036e5477fc6"
      },
      "source": [
        "theta = perceptron(X,y)\n",
        "print(theta)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "feature= 6  count= 2\n",
            "feature= 7  count= 1\n",
            "feature= 8  count= 3\n",
            "feature= 9  count= 2\n",
            "feature= 10  count= 9\n",
            "feature= 11  count= 1\n",
            "feature= 0  count= 0\n",
            "feature= 1  count= 0\n",
            "feature= 2  count= 0\n",
            "feature= 3  count= 0\n",
            "feature= 4  count= 0\n",
            "feature= 5  count= 0\n",
            "feature= 0  count= 5\n",
            "feature= 1  count= 9\n",
            "feature= 2  count= 2\n",
            "feature= 3  count= 3\n",
            "feature= 4  count= 5\n",
            "feature= 5  count= 1\n",
            "feature= 6  count= 0\n",
            "feature= 7  count= 0\n",
            "feature= 8  count= 0\n",
            "feature= 9  count= 0\n",
            "feature= 10  count= 0\n",
            "feature= 11  count= 0\n",
            "feature= 6  count= 8\n",
            "feature= 7  count= 1\n",
            "feature= 8  count= 5\n",
            "feature= 9  count= 3\n",
            "feature= 10  count= 1\n",
            "feature= 11  count= 1\n",
            "feature= 0  count= 0\n",
            "feature= 1  count= 0\n",
            "feature= 2  count= 0\n",
            "feature= 3  count= 0\n",
            "feature= 4  count= 0\n",
            "feature= 5  count= 0\n",
            "feature= 0  count= 7\n",
            "feature= 1  count= 7\n",
            "feature= 2  count= 8\n",
            "feature= 3  count= 5\n",
            "feature= 4  count= 8\n",
            "feature= 5  count= 1\n",
            "feature= 6  count= 0\n",
            "feature= 7  count= 0\n",
            "feature= 8  count= 0\n",
            "feature= 9  count= 0\n",
            "feature= 10  count= 0\n",
            "feature= 11  count= 0\n",
            "feature= 6  count= 2\n",
            "feature= 7  count= 1\n",
            "feature= 8  count= 3\n",
            "feature= 9  count= 2\n",
            "feature= 10  count= 9\n",
            "feature= 11  count= 1\n",
            "feature= 0  count= 0\n",
            "feature= 1  count= 0\n",
            "feature= 2  count= 0\n",
            "feature= 3  count= 0\n",
            "feature= 4  count= 0\n",
            "feature= 5  count= 0\n",
            "feature= 6  count= 8\n",
            "feature= 7  count= 1\n",
            "feature= 8  count= 5\n",
            "feature= 9  count= 3\n",
            "feature= 10  count= 1\n",
            "feature= 11  count= 1\n",
            "feature= 0  count= 0\n",
            "feature= 1  count= 0\n",
            "feature= 2  count= 0\n",
            "feature= 3  count= 0\n",
            "feature= 4  count= 0\n",
            "feature= 5  count= 0\n",
            "feature= 0  count= 7\n",
            "feature= 1  count= 7\n",
            "feature= 2  count= 8\n",
            "feature= 3  count= 5\n",
            "feature= 4  count= 8\n",
            "feature= 5  count= 1\n",
            "feature= 6  count= 0\n",
            "feature= 7  count= 0\n",
            "feature= 8  count= 0\n",
            "feature= 9  count= 0\n",
            "feature= 10  count= 0\n",
            "feature= 11  count= 0\n",
            "feature= 6  count= 2\n",
            "feature= 7  count= 1\n",
            "feature= 8  count= 3\n",
            "feature= 9  count= 2\n",
            "feature= 10  count= 9\n",
            "feature= 11  count= 1\n",
            "feature= 0  count= 0\n",
            "feature= 1  count= 0\n",
            "feature= 2  count= 0\n",
            "feature= 3  count= 0\n",
            "feature= 4  count= 0\n",
            "feature= 5  count= 0\n",
            "feature= 6  count= 6\n",
            "feature= 7  count= 5\n",
            "feature= 8  count= 3\n",
            "feature= 9  count= 8\n",
            "feature= 10  count= 1\n",
            "feature= 11  count= 1\n",
            "feature= 0  count= 0\n",
            "feature= 1  count= 0\n",
            "feature= 2  count= 0\n",
            "feature= 3  count= 0\n",
            "feature= 4  count= 0\n",
            "feature= 5  count= 0\n",
            "feature= 0  count= 5\n",
            "feature= 1  count= 9\n",
            "feature= 2  count= 2\n",
            "feature= 3  count= 3\n",
            "feature= 4  count= 5\n",
            "feature= 5  count= 1\n",
            "feature= 6  count= 0\n",
            "feature= 7  count= 0\n",
            "feature= 8  count= 0\n",
            "feature= 9  count= 0\n",
            "feature= 10  count= 0\n",
            "feature= 11  count= 0\n",
            "feature= 6  count= 6\n",
            "feature= 7  count= 5\n",
            "feature= 8  count= 3\n",
            "feature= 9  count= 8\n",
            "feature= 10  count= 1\n",
            "feature= 11  count= 1\n",
            "feature= 0  count= 0\n",
            "feature= 1  count= 0\n",
            "feature= 2  count= 0\n",
            "feature= 3  count= 0\n",
            "feature= 4  count= 0\n",
            "feature= 5  count= 0\n",
            "feature= 0  count= 5\n",
            "feature= 1  count= 7\n",
            "feature= 2  count= 3\n",
            "feature= 3  count= 9\n",
            "feature= 4  count= 7\n",
            "feature= 5  count= 1\n",
            "feature= 6  count= 0\n",
            "feature= 7  count= 0\n",
            "feature= 8  count= 0\n",
            "feature= 9  count= 0\n",
            "feature= 10  count= 0\n",
            "feature= 11  count= 0\n",
            "feature= 6  count= 2\n",
            "feature= 7  count= 1\n",
            "feature= 8  count= 3\n",
            "feature= 9  count= 2\n",
            "feature= 10  count= 9\n",
            "feature= 11  count= 1\n",
            "feature= 0  count= 0\n",
            "feature= 1  count= 0\n",
            "feature= 2  count= 0\n",
            "feature= 3  count= 0\n",
            "feature= 4  count= 0\n",
            "feature= 5  count= 0\n",
            "feature= 6  count= 6\n",
            "feature= 7  count= 5\n",
            "feature= 8  count= 3\n",
            "feature= 9  count= 8\n",
            "feature= 10  count= 1\n",
            "feature= 11  count= 1\n",
            "feature= 0  count= 0\n",
            "feature= 1  count= 0\n",
            "feature= 2  count= 0\n",
            "feature= 3  count= 0\n",
            "feature= 4  count= 0\n",
            "feature= 5  count= 0\n",
            "feature= 0  count= 2\n",
            "feature= 1  count= 9\n",
            "feature= 2  count= 3\n",
            "feature= 3  count= 7\n",
            "feature= 4  count= 4\n",
            "feature= 5  count= 1\n",
            "feature= 6  count= 0\n",
            "feature= 7  count= 0\n",
            "feature= 8  count= 0\n",
            "feature= 9  count= 0\n",
            "feature= 10  count= 0\n",
            "feature= 11  count= 0\n",
            "tensor([-11.,  27.,  -5.,  -6.,  -4.,  -3.,  11., -27.,   5.,   6.,   4.,   3.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wssGWeyjGm8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}